{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Union, Tuple\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import TextEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ.get(\"QDRANT_URL\", \"http://localhost:6333\")\n",
    "QDRANT_API_KEY = os.environ.get(\"QDRANT_API_KEY\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import RetrievalMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"security_reports\",\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"market maker\", k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'keywords': [], '_id': 3795654633996337262, '_collection_name': 'security_reports'}, page_content=''),\n",
       " Document(metadata={'keywords': ['Clober', 'LOBSTER', 'order book', 'DEX', 'on-chain order matching', 'smart contract platforms', 'limit orders', 'market orders', 'decentralized', 'trustless'], '_id': 2862050251125280971, '_collection_name': 'security_reports'}, page_content=''),\n",
       " Document(metadata={'keywords': ['LooksRare', 'NFT', 'Marketplace', 'Exchange V2'], '_id': 3190201343570982494, '_collection_name': 'security_reports'}, page_content=''),\n",
       " Document(metadata={'keywords': [], '_id': 822428441679300944, '_collection_name': 'security_reports'}, page_content=''),\n",
       " Document(metadata={'keywords': ['LooksRare', 'NFT', 'Marketplace', 'Exchange V2'], '_id': 6693479944826529634, '_collection_name': 'security_reports'}, page_content='')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*  [{'keywords': ['UniswapX', 'Dutch auctions', 'on-chain liquidity', 'off-chain liquidity', 'MEV', 'gas-less swaps', 'signed orders', 'fillers'], '_id': 532385020180679105, '_collection_name': 'security_reports'}]\n",
      "*  [{'keywords': ['DeFi bonds', 'ERC20', 'DAOs', 'smart contracts', 'zero coupon bonds', 'collateral', 'payment tokens', 'maturity', 'bondholders', 'lenders', 'borrowers'], '_id': 6017722462144705737, '_collection_name': 'security_reports'}]\n",
      "*  [{'keywords': ['Sudoswap', 'AMM', 'NFT-to-token swaps', 'bonding curves', 'ERC721', 'ERC20'], '_id': 1191993240635384991, '_collection_name': 'security_reports'}]\n",
      "*  [{'keywords': ['zkEVM-Contracts', 'Polygon-Hermez', 'zkEVM'], '_id': 7156953667558977319, '_collection_name': 'security_reports'}]\n",
      "*  [{'keywords': ['Beethoven X', 'Sonic Staking', 'decentralized investment platform', 'capital-efficient', 'sustainable solutions'], '_id': 6347570626470248028, '_collection_name': 'security_reports'}]\n"
     ]
    }
   ],
   "source": [
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_vectorstore(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Tool to retrieve relevant docs relating to the user's query.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        search_results =  qdrant_client.query(\n",
    "            collection_name=\"security_reports\",\n",
    "            query_text=query,\n",
    "            limit=3\n",
    "        )\n",
    "\n",
    "        results = [r.metadata[\"document\"] for r in search_results]\n",
    "\n",
    "        return {\"ai_response\": results}\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving docs: {str(e)}\")\n",
    "        return {\"ai_response\": \"An error occurred while retrieving documents.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error retrieving docs: Unexpected Response: 400 (Bad Request)\n",
      "Raw response content:\n",
      "b'{\"status\":{\"error\":\"Wrong input: Vector with name fast-bge-small-en-v1.5 is not configured in this collection\"},\"time\":0.000029284}'\n"
     ]
    }
   ],
   "source": [
    "result = chat_with_vectorstore(\"I need security reports of contracts involving MEVs and auctionung systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filehandlers import extract_text_from_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-small-latest\"\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filehandlers import extract_from_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(doc: Any):\n",
    "    full_text = extract_text_from_doc(doc)\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert document classifier and keyword extractor.\n",
    "\n",
    "    Instructions:\n",
    "    - Given an input text, extract the most important keywords that will serve as metadata for auditors to find similar projects.\n",
    "    - Focus only on niche or sector-specific web3 keywords that uniquely describe the project's core offering.\n",
    "    - Exclude generic crypto terms (e.g., \"token\", \"hack\").\n",
    "    - Also extract the project's name by analyzing the text and include it among the keywords.\n",
    "    - Return your output as a comma-separated list of keywords.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    Example 1:\n",
    "    Text:\n",
    "    \"Collar is a completely non-custodial lending protocol that does not rely on liquidations to remain solvent. Collar is powered by solvers instead of liquidators as well as other DeFi primitives like Uniswap v3.\n",
    "    Keywords:\n",
    "    \"Collar\", \"lending\", \"Uniswap v3\"\n",
    "\n",
    "    Example 2:\n",
    "    Text:\n",
    "    \"Astaria is a NFT Collateralized Lending Market leveraging a novel 3AM Model.\n",
    "    Keywords:\n",
    "    \"Astaria\", \"NFT\", \"Lending\", \"Market\", \"3AM Model\"\n",
    "\n",
    "    Example 3:\n",
    "    Text:\n",
    "    \"Base is a secure and low-cost Ethereum layer-2 solution built to scale the userbase on-chain.\n",
    "    Solady is an open source project for gas optimized Solidity snippets.\n",
    "    Keywords:\n",
    "    \"Base\", \"Solady\", \"layer-2\"\n",
    "\n",
    "    Example 4:\n",
    "    Text:\n",
    "    \"Royco Protocol allows anyone to create a market around any on-chain transaction (or series of transactions). Using Royco, incentive providers may create intents to offer incentives to users to perform the transaction(s) and users may create intents to complete the transaction(s) and/or negotiate for more incentives.\n",
    "    Keywords:\n",
    "    \"Royco\", \"Market\", \"Incentive providers\"\n",
    "    \"\"\"\n",
    "\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": full_text,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    keywords = chat_response.choices[0].message.content.replace('\\n', ' ').replace('Keywords:', '').strip()\n",
    "    metadata = re.findall(r'\"([^\"]+)\"', keywords)\n",
    "    return keywords, metadata, full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords, metadata, full_text = extract_metadata(\"/home/j/web3/scoping-agent/files/storage-proofs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## About the Project\n",
      "\n",
      "### Context\n",
      "\n",
      "At Curve, we offer a Savings Vault for crvUSD, an ERC4626 token that allows earning\n",
      "a \"risk-free\" interest rate on the crvUSD stablecoin.\n",
      "\n",
      "When bridging scrvUSD cross-chain, the token loses its ERC4626 capabilities and becomes\n",
      "a plain ERC20 token that cannot be minted with nor redeemed using crvUSD.\n",
      "\n",
      "To address this problem, we opted to have secondary scrvUSD markets on all chains where scrvUSD can be redeemed.\n",
      "Since the price of the asset is not stable, we cannot use a \"simple\" [stableswap-ng](https://github.com/curvefi/stableswap-ng/blob/fd54b9a1a110d0e2e4f962583761d9e236b70967/contracts/main/CurveStableSwapNG.vy#L17) pool as the price\n",
      "of the asset would go up as the yield accrues. Fortunately, stableswap-ng supports \"oraclized\" assets,\n",
      "which means that we can use an oracle to provide the rate at which the price of the asset is increasing, ensuring that the pool works as expected.\n",
      "\n",
      "### Problem\n",
      "\n",
      "It is a hard problem to guarantee the correctness of the value provided by the oracle. If not precise enough, this can\n",
      "lead to MEV in the liquidity pool, at a loss for the liquidity providers. Even worse, if someone is able to manipulate\n",
      "this rate, it can lead to the pool being drained from one side.\n",
      "\n",
      "### Solution\n",
      "\n",
      "This project contains a solution that fetches scrvUSD vault parameters from Ethereum, and provides them on other\n",
      "chains, with the goal of being able to compute the growth rate in a safe (non-manipulable) and precise\n",
      "(no losses due to approximation) way. Furthermore, this oracle can allow creating stableswap-ng pools for other assets\n",
      "like USDC/scrvUSD, FRAX/scrvUSD, etc.\n",
      "\n",
      "### Implementation details\n",
      "\n",
      "Since the actual scrvUSD price is subject to changes on Ethereum,\n",
      "it's impossible to predict the price with 100% certainty on other networks unless the price is taken at a given timestamp or some assumptions on scrvUSD behavior are introduced.\\\n",
      "Hence, the initial version named `v0` simply replicates scrvUSD and returns the real price at some timestamp.\n",
      "Considering the price is always growing, `v0` can be used as a lower bound.\\\n",
      "`v1` introduces the assumption that no one interacts with scrvUSD further on.\n",
      "This means that rewards are being distributed as is, stopping at the end of the period.\n",
      "And no new deposits/withdrawals alter the rate.\n",
      "This already gives a great assumption to the price over the distribution period,\n",
      "because the rate is more or less stable over a 1-week period and small declines do not matter.\\\n",
      "`v2` adds an assumption that rewards denominated in crvUSD are equal across subsequent periods.\n",
      "This is considered to approximate when rates are stable over a long period of time,\n",
      "especially when the market is calm.\n",
      "\n",
      "### Blockhash oracle\n",
      "\n",
      "It is out of scope, so listing general assumptions:\n",
      "\n",
      "1. It can be updated frequently with a mainnet blockhash that is no older than, say, 30 minutes. The minimal delay is 64 blocks to avoid any potential mainnet reorg risks.\n",
      "2. It can __rarely__ provide an incorrect blockhash, but not an incorrect block number. Thus, a new update with a fresh block number will correct the parameters.\n",
      "\n",
      "For curious readers, native blockhash feed solutions (like OP stack [pre-compile](https://optimistic.etherscan.io/address/0x4200000000000000000000000000000000000015#readProxyContract)) will be used with LayerZero for other networks as a \"temporary\" solution.\n",
      "\n",
      "### Parameters\n",
      "\n",
      "Spread from fee makes it impossible to manipulate the pool with changes up to the fee.\n",
      "Taking the minimum pool fee as 1bps means that the oracle should not jump more than 1 bps per block.\n",
      "And for extra safety, take 0.5bps as a safe threshold.\\\n",
      "Smoothing is introduced for sudden updates, so the price slowly catches up with the price, while the pool is being arbitraged safely.\n",
      "Though, smoothing limits the upper bound of price growth.\n",
      "Therefore, we consider that scrvUSD will never be over 60% APR.\n",
      "\n",
      "Also, it is worth noting that the oracle is controlled by a DAO and its parameters can be changed by a vote.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'files/storage-proofs.txt'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_text_from_doc(\"files/storage-proofs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Curve',\n",
       " 'crvUSD',\n",
       " 'ERC4626',\n",
       " 'scrvUSD',\n",
       " 'stableswap-ng',\n",
       " 'oracle',\n",
       " 'MEV',\n",
       " 'vault',\n",
       " 'USDC',\n",
       " 'FRAX',\n",
       " 'DAO',\n",
       " 'LayerZero',\n",
       " 'OP stack',\n",
       " 'blockhash']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_condition = models.Filter(\n",
    "    should=[\n",
    "        models.FieldCondition(\n",
    "            key=\"metadata.keywords\", \n",
    "            match=models.MatchAny(any=metadata)\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "results = qdrant_client.scroll(\n",
    "    collection_name=\"security_reports\",\n",
    "    scroll_filter=filter_condition\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_records(obj):\n",
    "    \"\"\"\n",
    "    Recursively yields individual record objects from a nested structure\n",
    "    (e.g., a tuple containing lists, etc.)\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        for item in obj:\n",
    "            yield from flatten_records(item)\n",
    "    else:\n",
    "        yield obj\n",
    "\n",
    "def get_file_names(records: Any) -> List[str]:\n",
    "    file_names = set()\n",
    "    for record in flatten_records(records):\n",
    "        # Check if record has a 'payload' attribute or key.\n",
    "        if hasattr(record, 'payload'):\n",
    "            payload = record.payload\n",
    "        elif isinstance(record, dict) and \"payload\" in record:\n",
    "            payload = record[\"payload\"]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if isinstance(payload, dict) and \"file_name\" in payload:\n",
    "            file_names.add(payload[\"file_name\"])\n",
    "    return list(file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Record(id=119691268391443, payload={'mongo_id': '67ca0c10ed3fdfebeaf700d9', 'file_name': 'Fastlane-Spearbit-Security-Review-April-2024.pdf', 'metadata': {'keywords': ['Fastlane', 'Atlas', 'MEV', 'Execution Abstraction', 'Solvers', 'User Operation', 'bundler', 'auction system']}, 'chunk_index': 248, 'chunk_text': 'can be retrieved from function arguments already.\\ncontrol.codehash is used for the following check:\\nmodifier validControlHash() {\\nif (_control().codehash != _controlCodeHash()) {\\nrevert(\"ERR-EV008 InvalidCodeHash\");\\n}\\n_;\\n}\\nThis check is to account for the possibility of changing the code via selfdestruct? With Dencun upgrade, selfdestruct\\ncan destroy the code only when called in the creation tx (rollups and other EVM chains haven\\'t upgraded to Dencun\\nyet). However, there are cases where this check doesn\\'t provide any protection against:\\n• If control is a proxy, the implementation can change without changing its codehash.\\n• control can change its execution without using proxy by detecting which stage Atlas is in. By calling solver-\\nMetaTryCatch and checking it\\'s in lock state or not. Although, with this malicious behavior, it likely won\\'t be\\nused by an honest user or an honest dapp.', 'total_chunks': 414}, vector=None, shard_key=None, order_value=None),\n",
       "  Record(id=9624078776047590, payload={'mongo_id': '67ca0bfaed3fdfebeaf700d0', 'file_name': 'Uniswapx-Spearbit-Security-Review-July-2024.pdf', 'metadata': {'keywords': ['UniswapX', 'Dutch auctions', 'on-chain liquidity', 'off-chain liquidity', 'MEV', 'gas-less swaps', 'signed orders', 'fillers']}, 'chunk_index': 16, 'chunk_text': \"puts[i].token <= feeOutputs[i - 1].token) may be used for duplicate detection.\\nAbove would require modifications to the array types (e.g. currently PriorityInput must be converted to Input-\\nToken when scaling) and the implementation logic and is not recommended for this current version of the protocol.\\nUniswapX: Acknowledged.\\nSpearbit: Acknowledged.\\n5.2.2\\nShort circuit input and output scaling in the case that they won't be scaled\\nSeverity: Gas Optimization\\nContext: PriorityFeeLib.sol#L29, PriorityFeeLib.sol#L42\\nDescription: In the input and output PriorityFeeLib scale functions, we scale the provided amount accordingly\\nto a priorityFee based scaling mechanism:\\namount: input.amount.mulDivDown((MPS - scalingFactor), MPS),\\namount: output.amount.mulDivUp((MPS + (priorityFee * output.mpsPerPriorityFeeWei)), MPS),\", 'total_chunks': 29}, vector=None, shard_key=None, order_value=None),\n",
       "  Record(id=27485767576145535, payload={'mongo_id': '67ca0c10ed3fdfebeaf700d9', 'file_name': 'Fastlane-Spearbit-Security-Review-April-2024.pdf', 'metadata': {'keywords': ['Fastlane', 'Atlas', 'MEV', 'Execution Abstraction', 'Solvers', 'User Operation', 'bundler', 'auction system']}, 'chunk_index': 181, 'chunk_text': \"function _assign(address owner, uint256 amount, bool solverWon, bool bidFind) internal returns (bool\\nisDeficit) {\\n,!\\nif (amount == 0) {\\naccessData[owner].lastAccessedBlock = uint32(block.number); // still save on bidFind\\n} else {\\n// ...\\nif (!bidFind) {\\naData.lastAccessedBlock = uint32(block.number);\\n}\\n}\\n}\\nuint256 gasWaterMark = gasleft();\\nfunction _releaseSolverLock(/*...*/ , uint256 gasWaterMark, /*...*/ ) /*...*/ {\\n// ...\\nuint256 gasUsed = (gasWaterMark - gasleft() + 5000) * tx.gasprice;\\n// other action to increase gasUsed\\n_assign(/*...*/ , gasUsed, /*...*/ );\\n// gasUsed at least 5000 * tx.gasprice\\n// ...\\n}\\nfunction _settle(/*...*/ ) /*...*/ {\\n// ...\\nif (_deposits < _claims + _withdrawals) {\\nuint256 amountOwed = _claims + _withdrawals - _deposits;\\nif (_assign(/*...*/ , amountOwed, /*...*/ )) { /*...*/ } // amountOwed > 0 otherwise doesn't\\nend up here\\n,!\\n}\\n// ...\\n}\", 'total_chunks': 414}, vector=None, shard_key=None, order_value=None),\n",
       "  Record(id=28389335767724662, payload={'mongo_id': '67ca0c10ed3fdfebeaf700d9', 'file_name': 'Fastlane-Spearbit-Security-Review-April-2024.pdf', 'metadata': {'keywords': ['Fastlane', 'Atlas', 'MEV', 'Execution Abstraction', 'Solvers', 'User Operation', 'bundler', 'auction system']}, 'chunk_index': 388, 'chunk_text': 'This error is not specific and difficult to trace back to the cause.\\nfunction _verifyDApp(/*...*/ ) /*...*/ {\\n// ...\\nif (!bypassSignatoryCheck) {\\nreturn (false, ValidCallsResult.DAppSignatureInvalid); // not specific\\n}\\n// ...\\n}\\nRecommendation: Consider using a more specific error in _verifyDApp() for example DappNotEnabled.\\nFastlane: Solved in PR 231.\\nSpearbit: Verified.\\n5.5.65\\nFunction _verifyDApp() accesses signatories[] directly\\nSeverity: Informational\\nContext: AtlasVerification.sol#L307-L377, DAppIntegration.sol#L186-L189\\nDescription: The function _verifyDApp() accesses the array signatories[] directly. This array is part of the\\ncontract DAppIntegration. This exposes the implementation details.\\n127', 'total_chunks': 414}, vector=None, shard_key=None, order_value=None),\n",
       "  Record(id=31653833637681753, payload={'mongo_id': '67ca0c10ed3fdfebeaf700d9', 'file_name': 'Fastlane-Spearbit-Security-Review-April-2024.pdf', 'metadata': {'keywords': ['Fastlane', 'Atlas', 'MEV', 'Execution Abstraction', 'Solvers', 'User Operation', 'bundler', 'auction system']}, 'chunk_index': 215, 'chunk_text': '5.4.3\\nCode duplication in initializeGovernance()\\nSeverity: Gas Optimization\\nContext: DAppIntegration.sol#L57-L72, DAppIntegration.sol#L140-L145\\nDescription: initializeGovernance() function duplicates the code of _addSignatory(). The only difference is\\nthe error message.\\nfunction initializeGovernance(address controller) external {\\n// ...\\n// Add DAppControl gov as a signatory\\nbytes32 signatoryKey = keccak256(abi.encodePacked(controller, msg.sender));\\nif (signatories[signatoryKey]) revert AtlasErrors.OwnerActive();\\nsignatories[signatoryKey] = true;\\ndAppSignatories[controller].push(msg.sender);\\n...\\n}\\nfunction _addSignatory(address controller, address signatory) internal {\\nbytes32 signatoryKey = keccak256(abi.encodePacked(controller, signatory));\\nif (signatories[signatoryKey]) revert AtlasErrors.SignatoryActive();\\nsignatories[signatoryKey] = true;', 'total_chunks': 414}, vector=None, shard_key=None, order_value=None)],\n",
       " 43570310693150780)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fastlane-Spearbit-Security-Review-April-2024.pdf', 'Uniswapx-Spearbit-Security-Review-July-2024.pdf']\n"
     ]
    }
   ],
   "source": [
    "print(get_file_names(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"security_reports\",\n",
    "    embedding=embeddings,\n",
    "    content_payload_key=\"chunk_text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_with_vectorstore_prompt = (\n",
    "    \"You are a seasoned security expert specializing in web3 security and smart contract auditing.\"\n",
    "    \"The context a comprehensive vector store of security reports, which include detailed risk classifications, vulnerabilities, and technical assessments.\"\n",
    "    \"When you receive a question, provide a precise, evidence-based answer that directly references the retrieved context.\"\n",
    "    \"Your answer should be structured clearly (using bullet points, headers, or numbered lists as needed) and avoid generic or vague responses.\"\n",
    "    \"Context: {context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"mistral-large-latest\", model_provider=\"mistralai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_vectorstore(query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Tool to retrieve relevant docs relating to the user's query.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", chat_with_vectorstore_prompt),\n",
    "                    (\"human\", \"{input}\"),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "            chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "            results = chain.invoke({\"input\": query})\n",
    "\n",
    "            return {\"ai_response\": results}\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving docs: {str(e)}\")\n",
    "            return {\"ai_response\": \"An error occurred while retrieving documents.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Was there any reentrancy vulnerability in the fastlane report?\"\n",
    "result = chat_with_vectorstore(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, there is no explicit mention of a reentrancy vulnerability in the Fastlane Atlas security review report. Here’s a breakdown of the issues found and their severities:\\n\\n- **Critical Risk**: 0 issues\\n- **High Risk**: 15 issues\\n- **Medium Risk**: 25 issues\\n- **Low Risk**: 35 issues\\n- **Gas Optimizations**: 34 issues\\n- **Informational**: 81 issues\\n\\nGiven that reentrancy vulnerabilities are typically classified as high or critical risk due to their severe impact, the absence of any critical risk issues and the lack of specific mention of reentrancy in the high risk issues suggests that no reentrancy vulnerabilities were identified during the audit.\\n\\nHowever, without the detailed descriptions of each high risk issue, it's not possible to definitively conclude that no reentrancy vulnerabilities exist. The report would need to be reviewed in full to ensure that none of the high risk issues are related to reentrancy.\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['ai_response']['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from textwrap import dedent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_security_report(report_text):\n",
    "    \"\"\"\n",
    "    Format a security report into a clean, structured output\n",
    "    regardless of input format.\n",
    "    \n",
    "    Args:\n",
    "        report_text (str): The raw text of the security report\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted report as Markdown\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Remove excessive newlines and whitespace\n",
    "    cleaned_text = re.sub(r'\\n{3,}', '\\n\\n', report_text.strip())\n",
    "    \n",
    "    # Extract title/headline if present\n",
    "    title_match = re.search(r'^(.+?)(?=\\n|$)', cleaned_text)\n",
    "    title = title_match.group(0) if title_match else \"Security Vulnerability Report\"\n",
    "    \n",
    "    # Extract severity if present\n",
    "    severity_match = re.search(r'(?i)severity[\\s:]*(critical|high|medium|low|informational|info)', cleaned_text)\n",
    "    severity = severity_match.group(1).capitalize() if severity_match else None\n",
    "    \n",
    "    # Extract location/file path if present\n",
    "    location_match = re.search(r'(?i)(?:location|file|path|contract)[\\s:]*([\\w\\.\\/#\\-]+\\.(?:sol|js|ts|py|jsx|vue)(?:[#:][L\\d\\-]+)?)', cleaned_text)\n",
    "    location = location_match.group(1) if location_match else None\n",
    "    \n",
    "    # Build formatted output\n",
    "    formatted_output = f\"# {title}\\n\\n\"\n",
    "    \n",
    "    if severity or location:\n",
    "        formatted_output += \"## Overview\\n\\n\"\n",
    "        if severity:\n",
    "            formatted_output += f\"**Severity:** {severity}\\n\\n\"\n",
    "        if location:\n",
    "            formatted_output += f\"**Location:** `{location}`\\n\\n\"\n",
    "    \n",
    "    # Process the main content\n",
    "    # Remove markdown-like artifacts but keep structure\n",
    "    content = re.sub(r'(^|\\n)#+ ', r'\\1### ', cleaned_text)\n",
    "    \n",
    "    # Extract description section\n",
    "    description_match = re.search(r'(?i)## *description\\s*(.*?)(?=\\n##|\\Z)', content, re.DOTALL)\n",
    "    if description_match:\n",
    "        description = description_match.group(1).strip()\n",
    "        formatted_output += f\"## Description\\n\\n{description}\\n\\n\"\n",
    "    \n",
    "    # Extract impact section\n",
    "    impact_match = re.search(r'(?i)## *impact\\s*(.*?)(?=\\n##|\\Z)', content, re.DOTALL)\n",
    "    if impact_match:\n",
    "        impact = impact_match.group(1).strip()\n",
    "        formatted_output += f\"## Impact\\n\\n{impact}\\n\\n\"\n",
    "    \n",
    "    # Extract technical details\n",
    "    technical_match = re.search(r'(?i)## *(technical details|vulnerability details)\\s*(.*?)(?=\\n##|\\Z)', content, re.DOTALL)\n",
    "    if technical_match:\n",
    "        technical = technical_match.group(2).strip()\n",
    "        formatted_output += f\"## Technical Details\\n\\n{technical}\\n\\n\"\n",
    "    \n",
    "    # Extract mitigation\n",
    "    mitigation_match = re.search(r'(?i)## *(mitigation|recommendation|fix|remediation)\\s*(.*?)(?=\\n##|\\Z)', content, re.DOTALL)\n",
    "    if mitigation_match:\n",
    "        mitigation = mitigation_match.group(2).strip()\n",
    "        formatted_output += f\"## Mitigation\\n\\n{mitigation}\\n\\n\"\n",
    "    \n",
    "    # If we didn't extract structured sections, include the whole content\n",
    "    if not any([description_match, impact_match, technical_match, mitigation_match]):\n",
    "        # Remove the title part we already used\n",
    "        main_content = re.sub(r'^.+?\\n', '', cleaned_text, 1)\n",
    "        formatted_output += f\"## Details\\n\\n{main_content}\\n\"\n",
    "    \n",
    "    return formatted_output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_report = format_security_report(result['ai_response']['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Based on the provided context, there is no explicit mention of a reentrancy vulnerability in the Fastlane Atlas security review report. Here’s a breakdown of the issues found and their severities:\n",
      "\n",
      "## Details\n",
      "\n",
      "\n",
      "- **Critical Risk**: 0 issues\n",
      "- **High Risk**: 15 issues\n",
      "- **Medium Risk**: 25 issues\n",
      "- **Low Risk**: 35 issues\n",
      "- **Gas Optimizations**: 34 issues\n",
      "- **Informational**: 81 issues\n",
      "\n",
      "Given that reentrancy vulnerabilities are typically classified as high or critical risk due to their severe impact, the absence of any critical risk issues and the lack of specific mention of reentrancy in the high risk issues suggests that no reentrancy vulnerabilities were identified during the audit.\n",
      "\n",
      "However, without the detailed descriptions of each high risk issue, it's not possible to definitively conclude that no reentrancy vulnerabilities exist. The report would need to be reviewed in full to ensure that none of the high risk issues are related to reentrancy.\n"
     ]
    }
   ],
   "source": [
    "print(formatted_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
